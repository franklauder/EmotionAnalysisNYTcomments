---
title: "Emotion Analysysof New York Times Comments"
format: html
---

# Introduction

# Data Review


```{python}

import pandas as pd
import matplotlib as plt
import seaborn as sns
import re
import string

```




```{python}

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize, pos_tag
import emoji
import contractions
from nltk.downloader import download
import nltk.downloader






```

```{python}


nltk.download('punkt_tab')

```

```{python}

nltk.download('punkt')


```

```{python}


nltk.download('averaged_perceptron_tagger_eng')

```


```{python}


print(nyt_comments1718.info())

```



```{python}


print(nyt_comments1718.head())



```

```{python}
print(nyt_comments1718.tail())


```

The commentBody column is the text that emotion sentiment  will be extracted from.


<br>


```{python}

text_string=str(text)


```


```{python}


text=nyt_comments1718["commentBody"]


```

Check if text data has any missing values (NAs)

```{python}



print(text.isna().sum())


```

One missing value is detected.  This will be resolved in data pre-processing. 



<br>

Next, we'll Check if the text contains emojis

```{python}

emoji.emoji_count(text)

```

Two emojis appear in the text.  This will be resolved in pre-processing. 


We'll now check if hastags are in the text

```{python}

text_string=str(text)


```

```{python}

if re.search(r'#\w+', text_string):
    print("text contains hashtags.")
else:
    print("text does not contain hashtags.")




```

<br>

We will check if there is whitepace in our text

```{python}


if " " in text:
    print("The text contains a space.")
else:
    print("The text does not contain a space.")



```

```{python}


if re.search(r'\s', text):
    print("The text contains whitespace (using regex).")
else:
    print("The text does not contain whitespace (using regex).")


```

```{python}


if any(char in text for char in string.whitespace):
    print("The text contains various whitespace characters.")
else:
    print("The text does not contain any whitespace characters.")



```



# Pre-processing

<br>

Our next step is to prepare the text for tokenization.  We'kk build functions to accomplish the pre-processing steps. 

<br>





Since emojis were detected, we will need a function to handle this.  We do not wish to remove the emojis, instead we want to  retain the emotion that emojis carry.  Our function will accomplish this  by converting  emojis to their word meanings, i.e. converting an emoji  to "smiling_face_with_heart_eyes" style tokens
<br>

```{python}


def demojize_text(text: str) -> str:
    
    return emoji.demojize(text, delimiters=(" ", " "))


```


<br>


Next,we  will build a function function for expanding contractions , i.e. "can't" will be converted to  "can not"
<br>

```{python}


def expand_contractions(text: str) -> str:
    return contractions.fix(text)



```

<br>


We will use a function for reducing 3+ repeated characters to 2 for vowels and consonants. An example is “soooo” which will be converted to  “soo” (or “so”).

<br>
```{python}

def normalize_elongations(text: str) -> str:
   
    return re.sub(r'(.)\1{2,}', r'\1\1', text)


```




<br>


Next, we'll handle punctuation.  There are punctuations like exclamation point (!) or question marks(?) that we do not removed as they convey emotion though we do wish to limit the lengh them. AS an example,  "!!!!!" will be limmited to at most "!!!" and "?????" limited to at most "???"
<br>

```{python}
def cap_punct_runs(text: str) -> str:
    text = re.sub(r'!{3,}', '!!!', text)
    text = re.sub(r'\?{3,}', '???', text)
    return text

```

<br>


To help with our emotion extraction, we wish to transform small numbers to their word meanings.  We will limit our range from  -3 to 20. 
<br>
```{python}


def number_to_words_if_helpful(token: str, enable=True):
    
    if not enable:
        return token
    if token.isdigit():
        try:
            n = int(token)
            if -20 <= n <= 20:  # small window is usually enough
                # Lightweight mapping to avoid external deps
                small = {
                    0:"zero",1:"one",2:"two",3:"three",4:"four",5:"five",6:"six",
                    7:"seven",8:"eight",9:"nine",10:"ten",11:"eleven",12:"twelve",
                    13:"thirteen",14:"fourteen",15:"fifteen",16:"sixteen",
                    17:"seventeen",18:"eighteen",19:"nineteen",20:"twenty",
                    -1:"minus one",-2:"minus two",-3:"minus three"
                }
                return small.get(n, token)
        except:
            pass
    return token


```

<br>
This next function maps POS tags to wordnet POS for lemmatiztion
<br>

```{python}


def get_wordnet_pos(treebank_tag):
    # map POS tags to wordnet POS for lemmatization
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN


```


```{python}



def replace_specials(text: str) -> str:
    text = URL_RE.sub(" <URL> ", text)
    text = EMAIL_RE.sub(" <EMAIL> ", text)
    text = MENTION_RE.sub(" <USER> ", text)
    return text



```


<br>

```{python}



stop_words=stopwords.words('english')
print(stop_words[:10])



```

<br>

Above, is a small sample of stop words. Some words  do not carry carry useful information such as words like "the", "and", or "is". Because they do not add value to Natural Language Processing, we use the stop word list to remove them and keep the mopre important words which are content rich.  An exception are stop words that affect emotion and intensity.  Such words are negators, words or phrases that express negation, meaning something is untrue, not happening, or of zero quantity(no, not never .  Other words are intensfiers, adverbs or adjectives that strengthen the meaning of other words (like very, really, or extremely), and modals,words that express modality, such as possibility, necessity, or ability (like, can, should, or must).  Our function will buildan  emotion-aware stoplist.


<br>


```{python}


def emotion_stopwords():
    sw = set(stopwords.words('english'))
    keep = {"no","not","nor","never","none","nobody","nothing","nowhere",
            "very","really","too","so","quite","extremely","hardly","barely","scarcely"}
    return sw - keep


```

<br>


 Simple negation: add _NEG suffix until punctuation boundary

```{python}


def negation_scope(tokens):
    negators = {"no","not","never","n't"}
    punct_boundary = {".","!","?",";",":"}
    out = []
    negate = False
    for tok in tokens:
        lower = tok.lower()
        if lower in negators:
            out.append(lower)  # keep the negator itself
            negate = True
            continue
        if tok in punct_boundary:
            out.append(tok)
            negate = False
            continue
        out.append(f"{tok}_NEG" if negate else tok)
    return out



```

<br>

Lemmatization reduces words to their base form, but uses  For example, "organizations" becomes "organization" 
<br>

```{python}


lemmatizer = WordNetLemmatizer()
STOP_EMO = emotion_stopwords()


```

<br>

Now that we have built our functions the pre-processing of the text can begin.
<br>

First step is to replace missing values with an empty string.

<br>

```{python}

text=text.fillna("")


```

```{python}


print(text.isna().sum())


```


<br>

Next, the text is transformed to a string format as this is required for tokeniation.  Word tokenization splits sentences into individual words and punctuation.

<br>

```{python}

type(text)

```

```{python}


text = str(text)


```


```{python}

type(text)

```

<br>

Now that our text has been transformed to a string, whitespace will be removed from the text. 

<br>




```{python}



text = text.replace(" ", "")


```


```{python}


if " " in text:
    print("The text contains a space.")
else:
    print("The text does not contain a space.")



```


```{python}



if any(char in text for char in string.whitespace):
    print("The text contains various whitespace characters.")
else:
    print("The text does not contain any whitespace characters.")


```

```{python}


if re.search(r'\s', text):
    print("The text contains whitespace.")
else:
    print("The text does not contain whitespace.")


```






```{python}



def preprocess_for_emotions(text: str,
                            convert_small_ints_to_words: bool = True) -> dict:
    # 1) sanitize
    if text is None:
        text = ""
    text = str(text)
    text = text.replace("\r", " ").strip()
    text = re.sub(r"\s+", " ", text)

    # 2) emoji/emoticon → text
    text = demojize_text(text)

    # 3) contractions
    text = expand_contractions(text)

    # 4) lowercase
    text = text.lower()

    # 5) urls/mentions/hashtags
    #text = replace_specials(text)
    #text = tokenize_hashtags(text)

    # 6) elongations & punct runs
    text = normalize_elongations(text)
    text = cap_punct_runs(text)

    # 7) tokenize
    tokens = word_tokenize(text)

    # 7b) (optional) convert small integers to words
    if convert_small_ints_to_words:
        tokens = [number_to_words_if_helpful(t, enable=True) for t in tokens]

    # 8) pos tag & lemmatize
    tagged = pos_tag(tokens)
    lemmas = [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in tagged]

    # 9) stopword filtering (emotion-aware)
    keep_punct = {"!", "?", "...", "!!!", "???"}
    filtered = [w for w in lemmas if (w in keep_punct) or (w.isalpha() and w not in STOP_EMO) or (w.endswith("_neg"))]

    # 10) negation scope
    final_tokens = negation_scope(filtered)

    # 11) final cleaned string (if needed)
    cleaned_text = " ".join(final_tokens)

    return {
        "tokens": final_tokens,
        "cleaned_text": cleaned_text
    }




```


```{python}

df['prep'] = preprocess_for_emotions(text)



```

```{python}

df['prep'] = nyt_comments1718['commentBody'].fillna("").apply(lambda x: preprocess_for_emotions(x))


```




```{python}


#df['tokens'] = df['prep'].apply(lambda d: d['tokens'])
# df['cleaned'] = df['prep'].apply(lambda d: d['cleaned_text'])



```








```{python}

tokens=word_tokenize(text_string)

```

```{python}


text=text.astype(str)


```

```{python}



tokens_list=[word_tokenize(text) for text in text_list]


```

```{python}


tokens_sentences=nltk.sent_tokenize(text)



```



```{python}





```



```{python}





```



```{python}





```